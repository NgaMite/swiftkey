Pitch
========================================================
author: Kevin S. (https://sg.linkedin.com/in/kevinsiswandi)
date: June, 2016

Product Lifecycle
========================================================

This data product was created and refined through iterative processes with the following general stages:

- Getting and cleaning the data
- Exploratory analysis
- Using N-gram model to build a word prediction framework
- Developing a predictive text application in shiny

The source code of the product can be found here: https://github.com/Physicist91/swiftkey

Analytic components
========================================================

Goal                                             | Solution
-------------------------------------------------|---------------------------------------------------------------------------
To avoid storing the entire training data in memory  | Created a permanent corpus on disk from the (raw) textual data
To transform the data into the right structure for analysis | Cleaned the data prior to N-gram tokenization by various transformations on the corpus.
To have a word dictionary from which the N-gram can be referenced  | Created tables of trigrams, bigrams, and unigrams from the raw corpus.
Predict unseen N-grams                           | Employ Katz backoff method



Critical product decisions
========================================================

1. Only the blogs data in the English language are used to train the model. Different text sources have different styles of English and I prefer to keep the style particular to one source.
2. Only trigrams are used in the final model, as the unigrams and bigrams are already contained within the trigrams.
3. Single-count trigrams (many of which may not be meaningful) are discarded to reduce the data size and Katz backoff method are applied to predict unseen n-grams.


The shiny app
========================================================

```{r, echo=FALSE}
plot(cars)
```

